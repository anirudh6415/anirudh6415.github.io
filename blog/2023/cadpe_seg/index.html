<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Segmentation on CAD-PE Dataset. | Anirudh Iyengar K N </title> <meta name="author" content="Anirudh Iyengar K N"> <meta name="description" content="The segmentation is performed on CAD-PE dataset"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ganaberundacircle.png?f13666142c28aa19d2b51c6075c67417"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://anirudh6415.github.io/blog/2023/cadpe_seg/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Segmentation on CAD-PE Dataset.",
            "description": "The segmentation is performed on CAD-PE dataset",
            "published": "May 07, 2023",
            "authors": [
              
              {
                "author": "Anirudh Iyengar",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Post grad student,Arizona State Univeristy",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Anirudh Iyengar</span> K N </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Segmentation on CAD-PE Dataset.</h1> <p>The segmentation is performed on CAD-PE dataset</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#cad-pe-segmentation">CAD-PE Segmentation</a> </div> <div> <a href="#cad-pe-dataset">CAD-PE Dataset</a> </div> <div> <a href="#building-the-segmentation-dataset">Building the Segmentation Dataset</a> </div> <div> <a href="#model-architecture-of-unet">Model Architecture of UNET</a> </div> <div> <a href="#training-the-model">Training the model</a> </div> <div> <a href="#evaluating-the-results">Evaluating the Results</a> </div> </nav> </d-contents> <h2 id="cad-pe-segmentation">CAD-PE Segmentation</h2> <h4 id="unveiling-insights">Unveiling Insights</h4> <p>Welcome to our blog, where we delve into the fascinating world of segmentation. The segmentation is performed on <strong>CAD-PE Challenge dataset</strong> <d-cite key="gonzalez2020computer"></d-cite>. In the realm of <code class="language-plaintext highlighter-rouge">"computer-aided design(CAD)"</code>, the precision and efficent segementation plays a pivotal role.<br></p> <p>In this blog series<d-footnote>Beginner's Work and Request for Understanding<br> Please note that this blog and the work presented herein are the efforts of a beginner in the field of image processing. While every attempt has been made to ensure accuracy and provide valuable insights, there may be certain limitations or areas for improvement. If any inconveniences or shortcomings are encountered, I kindly request your understanding and forgiveness. This blog serves as a starting point for exploring the fascinating world of Image processing and computer vision, and I am eager to learn and grow from this experience. Your feedback and suggestions are greatly appreciated as they will contribute to my growth as a learner and researcher. Thank you for your support and understanding.</d-footnote>, we will embark on an exciting journey to understand the challenges and intricacies of segmenting CAD-PE data. Whether you are a beginner or an experienced practitioner, we aim to provide valuable insights and practical guidance to enhance your understanding and proficiency in CAD-PE segmentation. Throught this blog, we will discuss various aspects of segementation,including data preprocessing, feature extraction, and model architectures. Moreover, we will dive into the evaluation metrics commonly used in assessing the performance of segmentation algorithms.</p> <p>Join me as we unravel the complexities of segmentation on CAD-PE dataset, empowering you to leverage this knowledge in your research, industry projects, or even personal endeavours. So, fasten your seatbelts and get ready to explore the world of segmentation like never before!!.Let’s unlock the hidden potential within Deep learning models and unleash their power.<br></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/blog1/cadpe.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/blog1/cadpemask.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Unveiling the Hidden Layers: A GIF showcasing the CAD image and its corresponding ground truth mask. </div> <hr> <h2 id="cad-pe-dataset">CAD-PE Dataset</h2> <p>The first step involved is exploring the dataset. The dataset involves <strong>91 patients CT scans</strong>. Each CT scan consists of some around 400 to 500 slices on average. Dividing the CT scans of the 91 patients into individual slices. This process allowed us to extract <strong>41,256 slices</strong> in total, which will serve as the foundation for our segmentation endeavors.</p> <p>Each slice within the CAD-PE dataset represents a two-dimensional image capturing a specific cross-section of the patients’ anatomy. These slices provide crucial insights into the internal structures and organs, enabling medical professionals and researchers to diagnose and study various conditions and diseases.</p> <hr> <h2 id="building-the-segmentation-dataset">Building the Segmentation Dataset</h2> <h4 id="slice-level-segmentation">Slice-Level Segmentation</h4> <p>In order to perform slice-level segmentation on the CAD-PE dataset, we will create a custom dataset named “segmentation_dataset”. This dataset will serve as the foundation for training and evaluating our segmentation algorithms.</p> <p>To begin, we will divide the available data randomly into three sets: training, validation, and testing. The training set will contain 80% of the data, while the validation and testing sets will each consist of 10% of the data. This division ensures a balanced distribution of slices across the different sets, enabling us to train and assess the performance of our models effectively.</p> <p>To handle the dataset efficiently, we will utilize filepaths to access the slices. Each slice within the CAD-PE dataset will be normalized to have pixel values ranging between 0 and 1. This normalization step ensures consistency and facilitates optimal model performance during training.</p> <p>Moreover, the input images and corresponding segmentation masks will be processed to adhere to the requirements of slice-level segmentation. The input images will be converted into single-channel representations, while the segmentation masks will be transformed into binary masks, consisting of only 0s and 1s. These binary masks serve as ground truth annotations for the presence or absence of the target structures within the slices.</p> <p>To facilitate seamless integration with deep learning frameworks, such as PyTorch, the slices will be transformed into tensors.</p> <p>The snippet code provided below showcases a high-level implementation for building the segmentation_dataset:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">segmentation_dataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">image_filenames</span><span class="p">,</span><span class="n">mask_filenames</span><span class="p">,</span><span class="n">transforms</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
      <span class="n">self</span><span class="p">.</span><span class="n">image_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/CAD_PE_Challenge_Data/images/</span><span class="sh">"</span>
      <span class="n">self</span><span class="p">.</span><span class="n">mask_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/CAD_PE_Challenge_Data/masks/</span><span class="sh">"</span>
      <span class="n">self</span><span class="p">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span>
      <span class="n">self</span><span class="p">.</span><span class="n">image_filenames</span> <span class="o">=</span> <span class="n">image_filenames</span>
      <span class="n">self</span><span class="p">.</span><span class="n">mask_filenames</span> <span class="o">=</span> <span class="n">mask_filenames</span>
      
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_filenames</span><span class="p">)</span>
  
  
  <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
      <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">image_filenames</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
      <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mask_dir</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">mask_filenames</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
      
      <span class="n">img</span> <span class="o">=</span> <span class="nf">nor_image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
      <span class="n">label</span> <span class="o">=</span> <span class="nf">binary</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
          
      <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">label</span>
    </code></pre></figure> <hr> <h2 id="model-architecture-of-unet">Model Architecture of UNET</h2> <p>To perform slice-level segmentation on the CAD-PE dataset, we will utilize the UNet model <d-cite key="ronneberger2015u"></d-cite>, which serves as a fundamental and widely adopted architecture for segmentation tasks. The UNet model is an excellent choice for beginners and provides a solid foundation for exploring and understanding segmentation techniques. Its simplicity and effectiveness make it a popular starting point in the field of computer vision and medical image analysis.</p> <p>UNet model combines the prinicple of convolutional neural networks(CNNs) and has encoder-decodr architecture. The encoder design allows the model to effectively capture features <strong><code class="language-plaintext highlighter-rouge">(What? in the image)</code></strong> and decoder desgin allows the model to upsample the features and recover the spatial resolution <strong><code class="language-plaintext highlighter-rouge">(Where? in the image)</code></strong>. Skip connections are incorporated to bridge the gap between the contracting and expanding paths, facilitating the propagation of low-level and high-level features.</p> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/blog1/u-net-architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 2: UNet architecture. </div> <hr> <h2 id="training-the-model">Training the model</h2> <p>In the quest for accurate segmentation masks on the CAD-PE dataset, training a model becomes a crucial step. Leveraging the advancements in deep learning, we can harness the power of neural networks to tackle the challenging task of CAD-PE segmentation.</p> <p>To begin the training process, we will employ the <code class="language-plaintext highlighter-rouge">binary cross entropy with logits</code> as the loss function. This loss function is particularly suited for segmentation tasks, as it compares the predicted segmentation masks with the ground truth masks, encouraging the model to accurately classify each pixel as belonging to the target structure or not.</p> <p>For optimization, we will utilize the <code class="language-plaintext highlighter-rouge">Adam optimizer</code>, a popular choice for training deep learning models. With a <code class="language-plaintext highlighter-rouge">learning rate of 0.001</code>, the Adam optimizer dynamically adjusts the learning rate during training, optimizing the model’s performance and convergence.</p> <p>To evaluate the quality of the segmentation results, we will employ the <code class="language-plaintext highlighter-rouge">Dice coefficient</code> as the evaluation metric. The Dice coefficient measures the overlap between the predicted and ground truth segmentation masks, providing a quantitative assessment of the model’s performance. A higher Dice coefficient indicates a better segmentation result, with values ranging from 0 (no overlap) to 1 (perfect overlap).</p> <p>During the training process, the model will iteratively learn from the CAD-PE dataset, updating its parameters to minimize the loss function. By iteratively feeding the input slices and corresponding ground truth masks to the model, it will gradually learn to accurately segment the structures of interest.</p> <p>The code snippet provided below showcases a high-level implementation for training the model for CAD-PE segmentation:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">train_loader</span><span class="p">,</span><span class="n">val_loader</span><span class="p">):</span>

    <span class="c1">#Define the loss function,optimizer, dice metric.
</span>    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BCEWithLogitsLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
    <span class="n">dice</span> <span class="o">=</span> <span class="nc">Dice</span><span class="p">(</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">micro</span><span class="sh">'</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">avg_test_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">avg_dice</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">avg_dice_val</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="c1"># Training on Training set
</span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            
            <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
            <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
          
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
            
            <span class="n">train_dice</span> <span class="o">=</span> <span class="nf">dice</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

            
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

            <span class="n">train_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            <span class="n">avg_dice</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">train_dice</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            
            
        <span class="c1"># evaluate on validation set
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">val_loader</span><span class="p">):</span>
                <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
                
                <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
                <span class="n">val_dice</span> <span class="o">=</span> <span class="nf">dice</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

                <span class="n">val_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">avg_dice_val</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">val_dice</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            
            <span class="n">val_loss_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
            <span class="n">avg_dice_val_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">avg_dice_val</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">avg_dice_val</span><span class="p">)</span>

       
        <span class="n">train_loss_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
        <span class="n">avg_dice_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">avg_dice</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">avg_dice</span><span class="p">)</span>

        <span class="c1"># print average losses and dice coefficients for the epoch
</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s"> | Train Loss: </span><span class="si">{</span><span class="n">train_loss_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> | Val Loss: </span><span class="si">{</span><span class="n">val_loss_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> |Dice Coefficient: </span><span class="si">{</span><span class="n">avg_dice_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> | Val Dice Coefficient: </span><span class="si">{</span><span class="n">avg_dice_val_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <p>By following this training process, we unlock the potential of deep learning to achieve accurate and reliable segmentation on the CAD-PE dataset. The model progressively learns to differentiate and classify the structures of interest.</p> <h2 id="evaluating-the-results">Evaluating the Results</h2> <p>After training the UNet model on the CAD-PE dataset, it is time to evaluate the achieved results. The Dice coefficient, a widely used metric for segmentation tasks, provides a quantitative measure of the model’s performance.</p> <p>On the training set, after <code class="language-plaintext highlighter-rouge">training for 30 epochs</code>, the UNet model achieved an impressive Dice coefficient of <strong>0.88197</strong>. This indicates a significant overlap between the predicted segmentation masks and the ground truth masks, highlighting the model’s ability to accurately capture the target structures within the CAD-PE slices.</p> <p>To further assess the generalization capability of the model, it is essential to evaluate its performance on the test set. On the <code class="language-plaintext highlighter-rouge">test set</code>, the UNet model attained a Dice coefficient of <strong>0.70431</strong>. Although slightly lower than the training set performance, it still demonstrates the model’s effectiveness in accurately segmenting the target structures in previously unseen data.</p> <p>To improve the model’s performance, several avenues can be explored. Firstly, hyperparameter tuning can be conducted by adjusting parameters such as learning rate, batch size, and regularization techniques. Fine-tuning these hyperparameters can potentially lead to better segmentation results.</p> <p>Additionally, data augmentation techniques can be employed to augment the training set. Techniques such as random rotations, translations, and scaling can increase the diversity of the training data, enhancing the model’s ability to generalize to unseen samples.</p> <p>Visualizing the model’s performance, the predicted segmentation masks can be compared side-by-side with the original ground truth masks.</p> <div style="text-align: center;"> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/blog1/predicted_result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Ground masks and Predicted masks. </div> <p>Overall, the results achieved by the UNet model showcase its potential in CAD-PE segmentation. With further experimentation, fine-tuning of hyperparameters, and augmentation of the training data, it is possible to unlock even better segmentation results. The UNet model serves as a foundation for future advancements in segmentation.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/cadpepapers.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"anirudh6415/anirudh6415.github.io","data-repo-id":"R_kgDOJe9PFw","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Anirudh Iyengar K N. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-bookshelf",title:"bookshelf",description:"",section:"Navigation",handler:()=>{window.location.href="/books/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"My github Repo",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-the-recommendation-what-to-shop-by-anirudh-narayana-medium",title:'The Recommendation: what to shop !!!!!! | by Anirudh Narayana | Medium <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"Before deepening the dataset and using it, let\u2019s first understand what recommendation systems are and why they are more relevant than ever. Recommendations systems are the algorithms that predict and\u2026",section:"Posts",handler:()=>{window.open("https://medium.com/@akaniyar/the-recommendation-what-to-shop-42bd2bacc551","_blank")}},{id:"post-tech-bits-amp-bytes-doing-nothing-by-anirudh-narayana-medium",title:'\ud83c\udf6bTech Bits & Bytes: Doing Nothing | by Anirudh Narayana | Medium <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"I\u2019m excited to start a new writing journey on Medium! \ud83c\udf89 Each week, I\u2019ll be sharing concise, byte-sized posts covering: \u2705 Emerging AI & ML technologies \ud83e\udde0\u2705 Fundamental concepts in AI, ML & Data\u2026",section:"Posts",handler:()=>{window.open("https://medium.com/@akaniyar/tech-bits-bytes-doing-nothing-7abc4d4687d9","_blank")}},{id:"post-2404-19756-kan-kolmogorov-arnold-networks",title:'[2404.19756] KAN: Kolmogorov-Arnold Networks <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"Abstract page for arXiv paper 2404.19756: KAN: Kolmogorov-Arnold Networks",section:"Posts",handler:()=>{window.open("https://arxiv.org/abs/2404.19756","_blank")}},{id:"post-ilsp-solutions",title:"ILSP Solutions",description:"An Introduction to Statistical Learning with Applications in Python (ISLP) Solutions",section:"Posts",handler:()=>{window.location.href="/blog/2024/ISLP_Solutions/"}},{id:"post-object-detection-on-argoversehd-dataset-exploring-yolov8-models",title:"Object Detection on Argoversehd Dataset - Exploring YOLOv8 Models",description:"Object detection is performed on ArgoverseHD-Dataset",section:"Posts",handler:()=>{window.location.href="/blog/2023/objectdetyolo/"}},{id:"post-segmentation-on-cad-pe-dataset",title:"Segmentation on CAD-PE Dataset.",description:"The segmentation is performed on CAD-PE dataset",section:"Posts",handler:()=>{window.location.href="/blog/2023/cadpe_seg/"}},{id:"post-\u0ca8\u0ca8\u0ccd\u0ca8-\u0c95\u0cb5\u0cbf\u0ca4\u0cc6\u0c97\u0cb3\u0cc1-\u0cad\u0cbe\u0cb5\u0ca8\u0cc6\u0c97\u0cb3-\u0c95\u0ca8\u0cb8\u0cc1\u0c97\u0cb3\u0cc1",title:"\u0ca8\u0ca8\u0ccd\u0ca8 \u0c95\u0cb5\u0cbf\u0ca4\u0cc6\u0c97\u0cb3\u0cc1-\u0cad\u0cbe\u0cb5\u0ca8\u0cc6\u0c97\u0cb3 \u0c95\u0ca8\u0cb8\u0cc1\u0c97\u0cb3\u0cc1",description:"\u0c88 \u0cac\u0ccd\u0cb2\u0cbe\u0c97\u0ccd \u0caa\u0ccb\u0cb8\u0ccd\u0c9f\u0ccd\u200c\u0ca8\u0cb2\u0ccd\u0cb2\u0cbf \u0ca8\u0cc0\u0cb5\u0cc1 \u0c95\u0ca8\u0cb8\u0cc1\u0c97\u0cb3 \u0cb8\u0ccd\u0cb5\u0cb0\u0cc2\u0caa\u0ca6 \u0c95\u0cb5\u0cbf\u0ca4\u0cc6\u0c97\u0cb3\u0ca8\u0ccd\u0ca8\u0cc1 \u0c95\u0c82\u0ca1\u0cc1\u0cb9\u0cbf\u0ca1\u0cbf\u0caf\u0cc1\u0cb5 \u0c85\u0ca8\u0cc1\u0cad\u0cb5\u0c95\u0ccd\u0c95\u0cc6 \u0c95\u0cb0\u0cc6\u0ca6\u0cca\u0caf\u0ccd\u0caf\u0cc1\u0ca4\u0ccd\u0ca4\u0cc7\u0cb5\u0cc6",section:"Posts",handler:()=>{window.location.href="/blog/2023/kavithegalu/"}},{id:"books-\u0c9a\u0cbf\u0ca6\u0c82\u0cac\u0cb0-\u0cb0\u0cb9\u0cb8\u0ccd\u0caf-chidambara-rahasya",title:"\u0c9a\u0cbf\u0ca6\u0c82\u0cac\u0cb0 \u0cb0\u0cb9\u0cb8\u0ccd\u0caf [Chidambara Rahasya]",description:"",section:"Books",handler:()=>{window.location.href="/books/Chidambara_Rahasya/"}},{id:"books-\u0c9c\u0cc1\u0c97\u0cbe\u0cb0\u0cbf-\u0c95\u0ccd\u0cb0\u0cbe\u0cb8\u0ccd-jugaari-cross",title:"\u0c9c\u0cc1\u0c97\u0cbe\u0cb0\u0cbf \u0c95\u0ccd\u0cb0\u0cbe\u0cb8\u0ccd | Jugaari Cross",description:"",section:"Books",handler:()=>{window.location.href="/books/Jugaari_Cross/"}},{id:"books-\u0c95\u0cb0\u0ccd\u0cb5\u0cbe\u0cb2\u0cca-karvalo",title:"\u0c95\u0cb0\u0ccd\u0cb5\u0cbe\u0cb2\u0cca | Karvalo",description:"",section:"Books",handler:()=>{window.location.href="/books/karvalo/"}},{id:"news-chs-student-research-symposium",title:"CHS Student Research Symposium",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-joined-jliang-research-lab-sparkles-smile",title:'Joined Jliang Research Lab <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-i-m-thrilled-to-announce-that-i-have-begun-a-new-role-as-a-graduate-student-assistant-gsa-at-arizona-state-university-asu",title:"I\u2019m thrilled to announce that I have begun a new role as a...",description:"",section:"News"},{id:"news-i-am-excited-to-share-that-i-have-started-a-new-role-as-a-machine-learning-intern-at-synapse-lab",title:"I am excited to share that I have started a new role as...",description:"",section:"News"},{id:"news-we-ve-submitted-two-papers-one-to-medai-journal-and-one-to-wacv-2025-proud-to-be-a-co-author",title:"\ud83c\udf89 We\u2019ve submitted two papers\u2014one to MedAI Journal and one to WACV 2025....",description:"",section:"News"},{id:"projects-modified-anytime-stereo-image-depth-estimation",title:"Modified Anytime Stereo Image Depth Estimation",description:"This is a Modified repository, it is based on the original Anytime Stereo Image Depth Estimation on Mobile Devices.",section:"Projects",handler:()=>{window.location.href="/projects/AnyNet/"}},{id:"projects-pulmonary-embolism-ct-scans-segmentation-and-localization",title:"Pulmonary Embolism CT scans Segmentation and Localization",description:"The code for segmentation and localization tasks in Computer Aided Detection for Pulmonary Embolism (CAD-PE).",section:"Projects",handler:()=>{window.location.href="/projects/CAD-PE-SegLoc/"}},{id:"projects-colonoscopy-segmentation",title:"Colonoscopy Segmentation",description:"Colonoscopy Image Analysis with UNet",section:"Projects",handler:()=>{window.location.href="/projects/Colon_seg/"}},{id:"projects-mask-dino-guided-distillation-for-semi-supervised-instance-segmentation",title:"Mask DINO + Guided Distillation for Semi-Supervised Instance Segmentation",description:"This is a Modified repository, it is based on the original Guided Distillation for Semi-Supervised Instance Segmentation. We merge the functionalities of Mask DINO and Guided Distillation for Semi-Supervised Instance Segmentation (based on Mask2Former) into a unified codebase.",section:"Projects",handler:()=>{window.location.href="/projects/Gudided_distillation/"}},{id:"projects-ai-hotel-reservation-prediction-system",title:"AI Hotel Reservation Prediction System",description:"ML-based hotel booking prediction system that leverages LightGBM for classification and MLflow for model tracking. The system is containerized using Docker and deployed on Google Cloud Platform (GCP) for scalability and ease of deployment.",section:"Projects",handler:()=>{window.location.href="/projects/Hotel_reservation_prediction/"}},{id:"projects-statistical-learning",title:"Statistical Learning",description:"Welcome to the ISLP Exercise repository! This repository contains my hands-on exercises related to the book Introduction to Statistical Learning with Python concepts implemented in Python using Jupiter Notebooks.",section:"Projects",handler:()=>{window.location.href="/projects/ISLP_learning/"}},{id:"projects-nih-chest-xray-dataset-classification-with-swin-transformer",title:"NIH Chest-Xray Dataset Classification with Swin Transformer",description:"the code for classifying Chest X-ray classification using the Swin Transformer Model.",section:"Projects",handler:()=>{window.location.href="/projects/NIH-CXR-Swin/"}},{id:"projects-cad-pe-classification-with-swin-transformer",title:"CAD-PE Classification with Swin Transformer",description:"the code for classifying Pulmonary Embolism (PE) in CT scans using the Swin Transformer model.",section:"Projects",handler:()=>{window.location.href="/projects/PE_ct_swin/"}},{id:"projects-yolov8-on-argoversehd-object-detection",title:"Yolov8 on ArgoverseHD Object Detection",description:"Object detection is performed on ArgoverseHD-Dataset - Exploring YOLOv8 Models",section:"Projects",handler:()=>{window.location.href="/projects/Yoloproject1/"}},{id:"projects-anime-recommendation-system-ai-based",title:"Anime Recommendation System - AI based",description:"A hybrid anime recommendation system that combines user preferences and content-based filtering for personalized recommendations. The system is containerized using Docker and deployed on Google Cloud Platform (GCP) with Kubernetes for scalability.",section:"Projects",handler:()=>{window.location.href="/projects/anime_recommendation_system/"}},{id:"projects-x-ray-image-clustering",title:"X-Ray Image Clustering",description:"clustering X-ray images using the K-Means algorithm",section:"Projects",handler:()=>{window.location.href="/projects/clustr_xrays/"}},{id:"projects-segmentation-on-jsrt",title:"Segmentation on JSRT",description:"instance segmentation model using the UNET to segment lungs and heart structures.",section:"Projects",handler:()=>{window.location.href="/projects/jsrt_segment/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6B%61%6E%69%79%61%72@%61%73%75.%65%64%75","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0001-2368-6303","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/anirudh6415","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/anirudhiyengar-kn","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>