<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://anirudh6415.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://anirudh6415.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-15T04:32:25+00:00</updated><id>https://anirudh6415.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">[2404.19756] KAN: Kolmogorov-Arnold Networks</title><link href="https://anirudh6415.github.io/blog/2024/240419756-kan-kolmogorov-arnold-networks/" rel="alternate" type="text/html" title="[2404.19756] KAN: Kolmogorov-Arnold Networks"/><published>2024-09-29T00:00:00+00:00</published><updated>2024-09-29T00:00:00+00:00</updated><id>https://anirudh6415.github.io/blog/2024/240419756-kan-kolmogorov-arnold-networks</id><content type="html" xml:base="https://anirudh6415.github.io/blog/2024/240419756-kan-kolmogorov-arnold-networks/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Abstract page for arXiv paper 2404.19756: KAN: Kolmogorov-Arnold Networks]]></summary></entry><entry><title type="html">ILSP Solutions</title><link href="https://anirudh6415.github.io/blog/2024/ISLP_Solutions/" rel="alternate" type="text/html" title="ILSP Solutions"/><published>2024-03-11T14:14:00+00:00</published><updated>2024-03-11T14:14:00+00:00</updated><id>https://anirudh6415.github.io/blog/2024/ISLP_Solutions</id><content type="html" xml:base="https://anirudh6415.github.io/blog/2024/ISLP_Solutions/"><![CDATA[<p align="center"> <a href="https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html"> <img src="https://images.squarespace-cdn.com/content/v1/5ff2adbe3fe4fe33db902812/8b373fbe-d1b4-4351-b803-0d3cd5bba1b0/ISLP_cover.png?format=100w" alt="ISLP book cover"/> </a> </p> <h2 id="an-introduction-to-statistical-learning-with-applications-in-python-islp-solutions">An Introduction to Statistical Learning with Applications in Python (ISLP) Solutions</h2> <p>The ISLP (Introduction to Statistical Learning), written by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor, is considered a gold standard, to use, for students pursuing prerequisites in machine learning. This book which is commonly found to be great in quality gets huge popularity as an introductory guide in the field of Machine Learning and Data Science. <a href="https://www.statlearning.com/"><b>Click here to get PDF</b></a></p> <p>The text covers mathematical and statistical theory of machine learning as well as applied labs in the programming language Python.</p> <p>Below, you’ll find exercise solutions written in JupyterLab using Python and Markdown, hosted on GitHub, serving as a demonstration of learning and reinforcement of concepts.</p> <h3 id="chapter-2-statistical-learning">Chapter 2: Statistical Learning</h3> <ul> <li><strong>Topics:</strong> What is statistical learning?, Basics of statistics</li> <li><strong>Lab:</strong> <a href="https://github.com/anirudh6415/ISLP_learning/blob/main/Exercises/02_Statistical_Learning/Ch02-statlearn-lab.ipynb"><b>Ch02-statlearn-lab</b></a></li> <li><strong>Exercise:</strong> <a href="https://github.com/anirudh6415/ISLP_learning/blob/main/Exercises/02_Statistical_Learning/Ch02-2.4-Exercise.ipynb"><b>Ch02-2.4-Exercise</b></a></li> </ul> <h3 id="chapter-3-linear-regression">Chapter 3: Linear Regression</h3> <ul> <li><strong>Topics:</strong> Linear regression, Multi Regression</li> <li><strong>Lab:</strong> <a href="https://github.com/anirudh6415/ISLP_learning/blob/main/Exercises/03_Linear_Regression/Ch03-linreg-lab.ipynb"><b>Ch03-linreg-lab</b></a></li> <li><strong>Exercise:</strong> <a href="https://github.com/anirudh6415/ISLP_learning/blob/main/Exercises/03_Linear_Regression/Ch03-3.7-Exercise.ipynb"><b>Ch03-3.7-Exercise</b></a></li> </ul> <h3 id="chapter-4-classification">Chapter 4: Classification</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-5-resampling-methods">Chapter 5: Resampling Methods</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-6-linear-model-selection-and-regularization">Chapter 6: Linear Model Selection and Regularization</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-7-moving-beyond-linearity">Chapter 7: Moving Beyond Linearity</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-8-tree-based-methods">Chapter 8: Tree-Based Methods</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-9-support-vector-machines">Chapter 9: Support Vector Machines</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-10-deep-learning">Chapter 10: Deep Learning</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-11-survival-analysis">Chapter 11: Survival Analysis</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-12-unsupervised-learning">Chapter 12: Unsupervised Learning</h3> <p><em>(Solutions not yet updated)</em></p> <h3 id="chapter-13-multiple-testing">Chapter 13: Multiple Testing</h3> <p><em>(Solutions not yet updated)</em></p>]]></content><author><name></name></author><category term="Statstical_Machine_Learning"/><category term="Python"/><category term="ML"/><category term="Statics"/><category term="Data_science"/><category term="ML"/><summary type="html"><![CDATA[An Introduction to Statistical Learning with Applications in Python (ISLP) Solutions]]></summary></entry><entry><title type="html">Object Detection on Argoversehd Dataset - Exploring YOLOv8 Models</title><link href="https://anirudh6415.github.io/blog/2023/objectdetyolo/" rel="alternate" type="text/html" title="Object Detection on Argoversehd Dataset - Exploring YOLOv8 Models"/><published>2023-06-07T14:14:00+00:00</published><updated>2023-06-07T14:14:00+00:00</updated><id>https://anirudh6415.github.io/blog/2023/objectdetyolo</id><content type="html" xml:base="https://anirudh6415.github.io/blog/2023/objectdetyolo/"><![CDATA[<h2 id="object-detection-using-yolov8-model">Object detection using YoloV8 model</h2> <h3 id="introduction">Introduction</h3> <p>Object detection, a subfield of computer vision, plays a crucial role in various domains, including driving. It enables the automated identification and localization of objects within images or videos, providing valuable insights and aiding in decision-making processes. While there are numerous models available for object detection, this blog focuses on YOLOv8, a state-of-the-art approach renowned for its accuracy and efficiency. In this experiment, we delve into the application of YOLOv8 models on the Argoversehd dataset, comparing the results obtained and exploring how YOLOv8 performs in this context.</p> <p>In the subsequent sections of this blog, we will dive deeper into the experimental setup, highlighting the key steps involved in training and evaluating the YOLOv8s and YOLOv8m models on the Argoversehd dataset. We will discuss the training process, including data preparation, model configuration, and hyperparameter tuning. Additionally, we will present the evaluation results and analyze how YOLOv8 performs compared to the original dataset. This comparative analysis will shed light on the strengths and limitations of YOLOv8 in the context of object detection on the Argoversehd dataset.</p> <p>Stay tuned as we embark on this exciting journey into the realm of object detection using YOLOv8 models. Through this experiment, we hope to gain insights that can contribute to the advancement of object detection techniques in the autonomus driving domain, fostering innovations that can benefit researchers alike.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> </p> <h3 id="argoverse-dataset">Argoverse Dataset</h3> <h4 id="preparation-for-yolov8-object-detection">Preparation for YOLOv8 Object Detection</h4> <p>The <a href="https://mtli.github.io/streaming/"><b>Argoverse dataset</b></a>, which forms the basis of our object detection experiment using YOLOv8 models, consists of a total of 66,954 images. The dataset is divided into three subsets: training, validation, and testing, with 39,384, 12,507, and 15,063 images, respectively. The training and validation subsets contain annotations in the COCO format, while the testing subset lacks ground truth annotations. In the absence of annotations, we will utilize the trained YOLOv8 models to predict and detect objects within the test images.</p> <p>The Argoverse dataset encompasses eight classes of objects, namely: “person,” “bicycle,” “car,” “motorcycle,” “bus,” “truck,” “traffic_light,” and “stop_sign.” These classes represent common objects typically found in vechile driving contexts.</p> <p>To prepare the dataset for YOLOv8, a specific directory structure is required.</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">root_data</span>
<span class="s">├── train</span>
<span class="s">│   ├── images</span>
<span class="s">│   │   ├── image1.jpg</span>
<span class="s">│   │   ├── image2.jpg</span>
<span class="s">│   │   └── ...</span>
<span class="s">│   └── labels</span>
<span class="s">│       ├── image1.txt</span>
<span class="s">│       ├── image2.txt</span>
<span class="s">│       └── ...</span>
<span class="s">├── val</span>
<span class="s">│   ├── images</span>
<span class="s">│   │   ├── image1.jpg</span>
<span class="s">│   │   ├── image2.jpg</span>
<span class="s">│   │   └── ...</span>
<span class="s">│   └── labels</span>
<span class="s">│       ├── image1.txt</span>
<span class="s">│       ├── image2.txt</span>
<span class="s">│       └── ...</span>
<span class="s">└── test</span>
    <span class="s">└── images</span>
        <span class="s">├── image1.jpg</span>
        <span class="s">├── image2.jpg</span>
        <span class="s">└── ...</span>

</code></pre></div></div> <p>To meet the YOLO format requirements, the annotations need to be converted from the COCO format to the YOLO format. Each linein the labels text file represents a single object annotation with its corresponding class ID, normalized coordinates, and dimensions.</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Image Label Text file</span><span class="pi">:</span>
<span class="s">class_id x_center y_center width height</span>
<span class="s">class_id x_center y_center width height</span>
<span class="s">class_id x_center y_center width height</span>
<span class="nn">...</span>

</code></pre></div></div> <p>Here is a sneak peek of the code used to convert the annotations to the YOLO format:</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="s">def convert_annotations_to_yolo_format(data, file_names, output_path)</span><span class="err">:</span>
    <span class="na">def get_img(filename)</span><span class="pi">:</span>
        <span class="s">for img in data['images']</span><span class="err">:</span>
            <span class="s">if img['file_name'] == filename</span><span class="err">:</span>
                <span class="s">return img</span>

    <span class="na">def get_img_ann(image_id)</span><span class="pi">:</span>
        <span class="s">img_ann = []</span>
        <span class="s">isFound = False</span>
        <span class="s">for ann in data['annotations']</span><span class="err">:</span>
            <span class="s">if ann['image_id'] == image_id</span><span class="err">:</span>
                <span class="s">img_ann.append(ann)</span>
                <span class="s">isFound = True</span>
        <span class="s">if isFound</span><span class="err">:</span>
            <span class="s">return img_ann</span>
        <span class="s">else</span><span class="err">:</span>
            <span class="s">return None</span>

    <span class="s">count = </span><span class="m">0</span>

    <span class="na">for filename in file_names</span><span class="pi">:</span>
        <span class="c1"># Extracting image</span>
        <span class="s">img = get_img(filename)</span>
        <span class="s">img_id = img['id']</span>
        <span class="s">img_w = img['width']</span>
        <span class="s">img_h = img['height']</span>

        <span class="s"># Get Annotations for this image</span>
        <span class="s">img_ann = get_img_ann(img_id)</span>
        <span class="s">fname = filename.split(".")[0]</span>
        <span class="s">if img_ann</span><span class="err">:</span>
            <span class="c1"># Opening file for the current image</span>
            <span class="s">file_object = open(f"{output_path}/{fname}.txt", "a")</span>
        <span class="s">if img_ann is not None</span><span class="err">:</span>
            <span class="na">for ann in img_ann</span><span class="pi">:</span>
                <span class="s">current_category = ann['category_id']</span>  <span class="c1"># As YOLO format labels start from 0</span>
                <span class="s">current_bbox = ann['bbox']</span>
                <span class="s">x = current_bbox[0]</span>
                <span class="s">y = current_bbox[1]</span>
                <span class="s">w = current_bbox[2]</span>
                <span class="s">h = current_bbox[3]</span>

                <span class="s"># Finding midpoints</span>
                <span class="s">x_centre = (x + (x+w))/2</span>
                <span class="s">y_centre = (y + (y+h))/2</span>

                <span class="s"># Normalization</span>
                <span class="s">x_centre = x_centre / img_w</span>
                <span class="s">y_centre = y_centre / img_h</span>
                <span class="s">w = w / img_w</span>
                <span class="s">h = h / img_h</span>

                <span class="s"># Limiting up to a fixed number of decimal places</span>
                <span class="s">x_centre = format(x_centre, '.6f')</span>
                <span class="s">y_centre = format(y_centre, '.6f')</span>
                <span class="s">w = format(w, '.6f')</span>
                <span class="s">h = format(h, '.6f')</span>

                <span class="s"># Writing the current object</span>
                <span class="s">file_object.write(f"{current_category} {x_centre} {y_centre} {w} {h}\n")</span>

        <span class="s">file_object.close()</span>
        <span class="s">count += </span><span class="m">1</span>
</code></pre></div></div> <p>After the conversion, the labels are saved as individual text files in the “labels” folder, corresponding to each image.</p> <p>With the dataset now prepared in the YOLOv8 format, we can proceed to train and evaluate the YOLOv8s and YOLOv8m models on the Agroverse dataset.</p> <h3 id="yolov8-architecture">YOLOv8 Architecture</h3> <p>YOLOv8 is an evolution of the YOLO (You Only Look Once) family of models, designed for efficient and accurate object detection.</p> <p>One significant update in YOLOv8 is its transition to anchor-free detection. Traditional object detection models often rely on predefined anchor boxes of different scales and aspect ratios to detect objects at various sizes. However, YOLOv8 takes a different approach by predicting the center of an object directly, rather than the offset from predefined anchor boxes.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog2/Anchor-free-480.webp 480w,/assets/img/blog2/Anchor-free-800.webp 800w,/assets/img/blog2/Anchor-free-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog2/Anchor-free.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 1: Difference between Anchor-free and Anchor based. <a herf=" https://www.nature.com/articles/s41598-021-02095-4"> <em> Image Source</em> </a> </div> <p>This anchor-free approach brings several advantages. Firstly, it simplifies the model architecture by removing the need for anchor boxes and associated calculations. This leads to a more streamlined and efficient network. Additionally, anchor-free detection allows for better localization accuracy, as the model directly predicts the object center with high precision.</p> <p>To visualize the YOLOv8 architecture and its anchor-free detection, we can refer to a detailed diagram created by GitHub user RangeKing (shown below). The diagram provides a comprehensive overview of the network’s structure and the flow of information through different layers.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog2/yolov8-480.webp 480w,/assets/img/blog2/yolov8-800.webp 800w,/assets/img/blog2/yolov8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog2/yolov8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2: YOLOv8 Architecture. </div> <p><strong>visualisation made by GitHub user</strong> <a href="https://github.com/RangeKing"><b>RangeKing</b></a></p> <p>By adopting anchor-free detection, YOLOv8 enhances object detection performance.</p> <h3 id="training-yolov8-on-agroverse-dataset">Training YOLOv8 on Agroverse Dataset</h3> <p>To train YOLOv8 on the Argoverse dataset, we need to create a <code class="language-plaintext highlighter-rouge">data.yaml</code> file and install the necessary dependencies. Here’s a step-by-step guide to training YOLOv8 on the Argoverse dataset:</p> <h5 id="create-the-datayaml-file"><strong>Create the <code class="language-plaintext highlighter-rouge">data.yaml</code> File</strong>:</h5> <p>Before training, we need to create a <code class="language-plaintext highlighter-rouge">data.yaml</code> file to specify the dataset’s configuration. The structure of the <code class="language-plaintext highlighter-rouge">data.yaml</code> file is as follows:</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">path</span><span class="pi">:</span> <span class="s">/your/root/path</span>
<span class="na">train</span><span class="pi">:</span> <span class="s">root/train/images/</span>
<span class="na">val</span><span class="pi">:</span> <span class="s">root/val/images/</span>
<span class="na">nc</span><span class="pi">:</span> <span class="s">number_of_classes</span>
<span class="na">names</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">class1</span><span class="pi">,</span> <span class="nv">class2</span><span class="pi">,</span> <span class="nv">...</span><span class="pi">,</span> <span class="nv">classN</span><span class="pi">]</span>
</code></pre></div></div> <p>Ensure that you replace <code class="language-plaintext highlighter-rouge">/your/root/path</code> with the actual root path of your dataset, <code class="language-plaintext highlighter-rouge">root/train/images/</code> with the path to the training images folder, <code class="language-plaintext highlighter-rouge">root/val/images/</code> with the path to the validation images folder, <code class="language-plaintext highlighter-rouge">number_of_classes</code> with the total number of classes in your dataset, and <code class="language-plaintext highlighter-rouge">[class1, class2, ..., classN]</code> with a list of the class names in string format.</p> <h5 id="install-dependencies-install-the-required-dependencies-by-running-the-following-command"><strong>Install Dependencies</strong>: Install the required dependencies by running the following command:</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">ultralytics</span>
</code></pre></div></div> <h5 id="import-yolo-and-load-the-model-import-the-yolo-class-from-the-ultralytics-package-and-load-the-yolov8-model-using-the-desired-pt-file"><strong>Import YOLO and Load the Model</strong>: Import the <code class="language-plaintext highlighter-rouge">YOLO</code> class from the <code class="language-plaintext highlighter-rouge">ultralytics</code> package and load the YOLOv8 model using the desired <code class="language-plaintext highlighter-rouge">.pt</code> file:</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">YOLO</span><span class="p">(</span><span class="sh">'</span><span class="s">yolov8s.pt</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">YOLO</code> class from <code class="language-plaintext highlighter-rouge">ultralytics</code> automatically downloads the required YOLOv8 models, such as <code class="language-plaintext highlighter-rouge">yolov8s</code> or <code class="language-plaintext highlighter-rouge">yolov8m</code>, based on the specified <code class="language-plaintext highlighter-rouge">.pt</code> file.</p> <h5 id="start-training-begin-the-training-process-by-calling-the-train-method-on-the-model-object-with-appropriate-arguments-heres-an-example-configuration"><strong>Start Training</strong>: Begin the training process by calling the <code class="language-plaintext highlighter-rouge">train</code> method on the <code class="language-plaintext highlighter-rouge">model</code> object with appropriate arguments. Here’s an example configuration:</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span>
   <span class="n">data</span><span class="o">=</span><span class="sh">'</span><span class="s">Argoverse.yaml</span><span class="sh">'</span><span class="p">,</span>
   <span class="n">imgsz</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
   <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
   <span class="n">batch</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
   <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
   <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">yolov8m_custom</span><span class="sh">'</span><span class="p">,</span>
   <span class="n">val</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
   <span class="n">project</span><span class="o">=</span><span class="sh">'</span><span class="s">yolov8m_custom_Argoverse</span><span class="sh">'</span><span class="p">,</span>
   <span class="n">save_period</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>
</code></pre></div></div> <p>In this example, we specify the <code class="language-plaintext highlighter-rouge">data</code> parameter as <code class="language-plaintext highlighter-rouge">'Argoverse.yaml'</code> to use the created <code class="language-plaintext highlighter-rouge">data.yaml</code> file. Adjust the other parameters such as <code class="language-plaintext highlighter-rouge">imgsz</code> (image size), <code class="language-plaintext highlighter-rouge">epochs</code> (number of training epochs), <code class="language-plaintext highlighter-rouge">batch</code> (batch size), <code class="language-plaintext highlighter-rouge">save</code> (whether to save checkpoints), <code class="language-plaintext highlighter-rouge">name</code> (name for the trained model), <code class="language-plaintext highlighter-rouge">val</code> (whether to evaluate on the validation set), <code class="language-plaintext highlighter-rouge">project</code> (project name for logging), and <code class="language-plaintext highlighter-rouge">save_period</code> (number of epochs between saving checkpoints) according to your requirements.</p> <h5 id="monitor-training-progress"><strong>Monitor Training Progress</strong>:</h5> <p>During training, the YOLO model will provide updates on the training loss, bounding box loss, mean Average Precision (mAP), etc.</p> <p>For more detailed information and additional training options, refer to the <a herf="https://docs.ultralytics.com/modes/train/"><b>YOLOv5 Train Mode Documentation</b></a> provided by Ultralytics.</p> <h3 id="testing-the-trained-yolov8-model">Testing the Trained YOLOv8 Model</h3> <p>After training the YOLOv8 model on the Argoverse dataset, it’s time to evaluate its performance on the test data. In this section, we will test the best trained YOLOv8s and YOLOv8m models on the test dataset.</p> <p>Firstly, the test data for Argoverse consists of individual images. To provide a more comprehensive evaluation, I converted 2000 frames of the test data into a video at 24 frames per second (fps). This video allows for a sequential analysis of the model’s object detection capabilities. Also predicted on whole test data.</p> <p>Here’s an example configaration :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="sh">'</span><span class="s">Yolov8/test_video.mp4</span><span class="sh">'</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># You Can also add path to your images
</span></code></pre></div></div> <p>Below are the videos showcasing original and the testing results of the YOLOv8s and YOLOv8m models on the test data: <strong>Test Video</strong></p> <div class="row mt-3"> <iframe width="1002" height="626" src="https://www.youtube.com/embed/SeRUThVhlc4" title="Test Video for testing YoloV8 model" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div class="caption"> <b> Original Test video.</b> </div> <p><strong>YOLOv8s Predicted Video</strong></p> <div class="row mt-3"> <iframe width="1002" height="626" src="https://www.youtube.com/embed/NMq17lLEHEw" title="Prediction of YoloV8s model" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div class="caption"> <b> YOLOV8s Prediction.</b> </div> <p><strong>YOLOv8m Predicted Video:</strong></p> <div class="row mt-3"> <iframe width="1002" height="626" src="https://www.youtube.com/embed/2_2clDwQSb0" title="Prediction of YOLOv8m model" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div class="caption"> <b> YOLOV8m Prediction.</b> </div> <p>By visually examining the test videos, we can observe how the YOLOv8 models detect and classify objects in the Argoverse test dataset. The models’ performance will be evident in their ability to accurately identify and localize objects of interest, such as people, bicycles, cars, motorcycles, buses, trucks, traffic lights, and stop signs.</p> <p>The models will output bounding boxes around the detected objects, along with their corresponding class labels and confidence scores.</p> <h3 id="analyzing-the-test-results">Analyzing the Test Results</h3> <p>After testing the YOLOv8 models on the Argoversehd dataset and evaluating the results, it is important to conduct a thorough analysis to gain insights into the performance of the models. This analysis involves both visual inspection and the use of quantitative metrics to assess the models’ effectiveness in object detection tasks.</p> <h4 id="visual-inspection">Visual Inspection:</h4> <p>Upon visually inspecting the test results, it becomes evident that the YOLOv8 models show promising performance in detecting and localizing objects. However, there are areas where the models exhibit limitations. For example, the models incorrectly identify certain objects as trucks and miss some instances of stop signs. These observations suggest that further improvements can be made by refining the training process and incorporating additional data.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog2/results-480.webp 480w,/assets/img/blog2/results-800.webp 800w,/assets/img/blog2/results-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog2/results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="quantitative-metrics">Quantitative Metrics:</h4> <p>The mean Average Precision (mAP) is a widely used metric for evaluating object detection models. The mAP measures the accuracy of object localization and classification. In the case of the YOLOv8 models trained on the Argoversehd dataset, the highest achieved mAP is 0.40, indicating good performance for certain instances. However, the average mAP typically falls within the range of 0.24 to 0.35. This implies that there is room for improvement in terms of the models’ overall accuracy and precision.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog2/F1_curve-480.webp 480w,/assets/img/blog2/F1_curve-800.webp 800w,/assets/img/blog2/F1_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog2/F1_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog2/PR_curve-480.webp 480w,/assets/img/blog2/PR_curve-800.webp 800w,/assets/img/blog2/PR_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog2/PR_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="confusion-matrix">Confusion Matrix:</h4> <p>A confusion matrix provides a detailed breakdown of the model’s performance across different object classes. By analyzing the confusion matrix, we can identify specific areas where the YOLOv8 models excel and areas where they struggle. In the case of the Argoversehd dataset, the YOLO models face challenges in accurately detecting small objects and occasionally misclassifying certain objects. To address these limitations, it is advisable to consider strategies such as increasing the amount of training data and conducting further model optimization.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog2/confusion_matrix-480.webp 480w,/assets/img/blog2/confusion_matrix-800.webp 800w,/assets/img/blog2/confusion_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog2/confusion_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="improving-model-performance">Improving Model Performance:</h4> <p>Based on the analysis of the test results, it is clear that there is room for improvement in the YOLOv8 models’ performance on the Argoversehd dataset. By implementing different strategies and iteratively training and evaluating the YOLOv8 models, it is possible to improve their object detection accuracy and address the specific challenges observed during testing on the Argoversehd dataset.</p> <p><strong><em>Footnote</em></strong>:</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><strong><em>Beginner’s Work and Request for Understanding</em></strong> - <em>Please note that this blog and the work presented herein are the efforts of a beginner in the field of image processing. While every attempt has been made to ensure accuracy and provide valuable insights, there may be certain limitations or areas for improvement. If any inconveniences or shortcomings are encountered, I kindly request your understanding and forgiveness. This blog serves as a starting point for exploring the fascinating world of Image processing and computer vision, and I am eager to learn and grow from this experience. Your feedback and suggestions are greatly appreciated as they will contribute to my growth as a learner and researcher. Thank you for your support and understanding.</em> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Anirudh Iyengar</name></author><category term="Object_detection"/><category term="Yolov8"/><category term="computer_vision"/><category term="image_processing"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[Object detection is performed on ArgoverseHD-Dataset]]></summary></entry><entry><title type="html">Segmentation on CAD-PE Dataset.</title><link href="https://anirudh6415.github.io/blog/2023/cadpe_seg/" rel="alternate" type="text/html" title="Segmentation on CAD-PE Dataset."/><published>2023-05-07T18:00:00+00:00</published><updated>2023-05-07T18:00:00+00:00</updated><id>https://anirudh6415.github.io/blog/2023/cadpe_seg</id><content type="html" xml:base="https://anirudh6415.github.io/blog/2023/cadpe_seg/"><![CDATA[<h2 id="cad-pe-segmentation">CAD-PE Segmentation</h2> <h4 id="unveiling-insights">Unveiling Insights</h4> <p>Welcome to our blog, where we delve into the fascinating world of segmentation. The segmentation is performed on <strong>CAD-PE Challenge dataset</strong> <d-cite key="gonzalez2020computer"></d-cite>. In the realm of <code class="language-plaintext highlighter-rouge">"computer-aided design(CAD)"</code>, the precision and efficent segementation plays a pivotal role.<br/></p> <p>In this blog series<d-footnote>Beginner's Work and Request for Understanding<br/> Please note that this blog and the work presented herein are the efforts of a beginner in the field of image processing. While every attempt has been made to ensure accuracy and provide valuable insights, there may be certain limitations or areas for improvement. If any inconveniences or shortcomings are encountered, I kindly request your understanding and forgiveness. This blog serves as a starting point for exploring the fascinating world of Image processing and computer vision, and I am eager to learn and grow from this experience. Your feedback and suggestions are greatly appreciated as they will contribute to my growth as a learner and researcher. Thank you for your support and understanding.</d-footnote>, we will embark on an exciting journey to understand the challenges and intricacies of segmenting CAD-PE data. Whether you are a beginner or an experienced practitioner, we aim to provide valuable insights and practical guidance to enhance your understanding and proficiency in CAD-PE segmentation. Throught this blog, we will discuss various aspects of segementation,including data preprocessing, feature extraction, and model architectures. Moreover, we will dive into the evaluation metrics commonly used in assessing the performance of segmentation algorithms.</p> <p>Join me as we unravel the complexities of segmentation on CAD-PE dataset, empowering you to leverage this knowledge in your research, industry projects, or even personal endeavours. So, fasten your seatbelts and get ready to explore the world of segmentation like never before!!.Let’s unlock the hidden potential within Deep learning models and unleash their power.<br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/cadpe-480.webp 480w,/assets/img/blog1/cadpe-800.webp 800w,/assets/img/blog1/cadpe-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog1/cadpe.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/cadpemask-480.webp 480w,/assets/img/blog1/cadpemask-800.webp 800w,/assets/img/blog1/cadpemask-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog1/cadpemask.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Unveiling the Hidden Layers: A GIF showcasing the CAD image and its corresponding ground truth mask. </div> <hr/> <h2 id="cad-pe-dataset">CAD-PE Dataset</h2> <p>The first step involved is exploring the dataset. The dataset involves <strong>91 patients CT scans</strong>. Each CT scan consists of some around 400 to 500 slices on average. Dividing the CT scans of the 91 patients into individual slices. This process allowed us to extract <strong>41,256 slices</strong> in total, which will serve as the foundation for our segmentation endeavors.</p> <p>Each slice within the CAD-PE dataset represents a two-dimensional image capturing a specific cross-section of the patients’ anatomy. These slices provide crucial insights into the internal structures and organs, enabling medical professionals and researchers to diagnose and study various conditions and diseases.</p> <hr/> <h2 id="building-the-segmentation-dataset">Building the Segmentation Dataset</h2> <h4 id="slice-level-segmentation">Slice-Level Segmentation</h4> <p>In order to perform slice-level segmentation on the CAD-PE dataset, we will create a custom dataset named “segmentation_dataset”. This dataset will serve as the foundation for training and evaluating our segmentation algorithms.</p> <p>To begin, we will divide the available data randomly into three sets: training, validation, and testing. The training set will contain 80% of the data, while the validation and testing sets will each consist of 10% of the data. This division ensures a balanced distribution of slices across the different sets, enabling us to train and assess the performance of our models effectively.</p> <p>To handle the dataset efficiently, we will utilize filepaths to access the slices. Each slice within the CAD-PE dataset will be normalized to have pixel values ranging between 0 and 1. This normalization step ensures consistency and facilitates optimal model performance during training.</p> <p>Moreover, the input images and corresponding segmentation masks will be processed to adhere to the requirements of slice-level segmentation. The input images will be converted into single-channel representations, while the segmentation masks will be transformed into binary masks, consisting of only 0s and 1s. These binary masks serve as ground truth annotations for the presence or absence of the target structures within the slices.</p> <p>To facilitate seamless integration with deep learning frameworks, such as PyTorch, the slices will be transformed into tensors.</p> <p>The snippet code provided below showcases a high-level implementation for building the segmentation_dataset:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">segmentation_dataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">image_filenames</span><span class="p">,</span><span class="n">mask_filenames</span><span class="p">,</span><span class="n">transforms</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
      <span class="n">self</span><span class="p">.</span><span class="n">image_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/CAD_PE_Challenge_Data/images/</span><span class="sh">"</span>
      <span class="n">self</span><span class="p">.</span><span class="n">mask_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/CAD_PE_Challenge_Data/masks/</span><span class="sh">"</span>
      <span class="n">self</span><span class="p">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span>
      <span class="n">self</span><span class="p">.</span><span class="n">image_filenames</span> <span class="o">=</span> <span class="n">image_filenames</span>
      <span class="n">self</span><span class="p">.</span><span class="n">mask_filenames</span> <span class="o">=</span> <span class="n">mask_filenames</span>
      
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_filenames</span><span class="p">)</span>
  
  
  <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
      <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">image_filenames</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
      <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mask_dir</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">mask_filenames</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
      
      <span class="n">img</span> <span class="o">=</span> <span class="nf">nor_image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
      <span class="n">label</span> <span class="o">=</span> <span class="nf">binary</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
          
      <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">label</span>
    </code></pre></figure> <hr/> <h2 id="model-architecture-of-unet">Model Architecture of UNET</h2> <p>To perform slice-level segmentation on the CAD-PE dataset, we will utilize the UNet model <d-cite key="ronneberger2015u"></d-cite>, which serves as a fundamental and widely adopted architecture for segmentation tasks. The UNet model is an excellent choice for beginners and provides a solid foundation for exploring and understanding segmentation techniques. Its simplicity and effectiveness make it a popular starting point in the field of computer vision and medical image analysis.</p> <p>UNet model combines the prinicple of convolutional neural networks(CNNs) and has encoder-decodr architecture. The encoder design allows the model to effectively capture features <strong><code class="language-plaintext highlighter-rouge">(What? in the image)</code></strong> and decoder desgin allows the model to upsample the features and recover the spatial resolution <strong><code class="language-plaintext highlighter-rouge">(Where? in the image)</code></strong>. Skip connections are incorporated to bridge the gap between the contracting and expanding paths, facilitating the propagation of low-level and high-level features.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/u-net-architecture-480.webp 480w,/assets/img/blog1/u-net-architecture-800.webp 800w,/assets/img/blog1/u-net-architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog1/u-net-architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2: UNet architecture. </div> <hr/> <h2 id="training-the-model">Training the model</h2> <p>In the quest for accurate segmentation masks on the CAD-PE dataset, training a model becomes a crucial step. Leveraging the advancements in deep learning, we can harness the power of neural networks to tackle the challenging task of CAD-PE segmentation.</p> <p>To begin the training process, we will employ the <code class="language-plaintext highlighter-rouge">binary cross entropy with logits</code> as the loss function. This loss function is particularly suited for segmentation tasks, as it compares the predicted segmentation masks with the ground truth masks, encouraging the model to accurately classify each pixel as belonging to the target structure or not.</p> <p>For optimization, we will utilize the <code class="language-plaintext highlighter-rouge">Adam optimizer</code>, a popular choice for training deep learning models. With a <code class="language-plaintext highlighter-rouge">learning rate of 0.001</code>, the Adam optimizer dynamically adjusts the learning rate during training, optimizing the model’s performance and convergence.</p> <p>To evaluate the quality of the segmentation results, we will employ the <code class="language-plaintext highlighter-rouge">Dice coefficient</code> as the evaluation metric. The Dice coefficient measures the overlap between the predicted and ground truth segmentation masks, providing a quantitative assessment of the model’s performance. A higher Dice coefficient indicates a better segmentation result, with values ranging from 0 (no overlap) to 1 (perfect overlap).</p> <p>During the training process, the model will iteratively learn from the CAD-PE dataset, updating its parameters to minimize the loss function. By iteratively feeding the input slices and corresponding ground truth masks to the model, it will gradually learn to accurately segment the structures of interest.</p> <p>The code snippet provided below showcases a high-level implementation for training the model for CAD-PE segmentation:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">train_loader</span><span class="p">,</span><span class="n">val_loader</span><span class="p">):</span>

    <span class="c1">#Define the loss function,optimizer, dice metric.
</span>    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BCEWithLogitsLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>
    <span class="n">dice</span> <span class="o">=</span> <span class="nc">Dice</span><span class="p">(</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">micro</span><span class="sh">'</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">avg_test_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">avg_dice</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">avg_dice_val</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="c1"># Training on Training set
</span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            
            <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
            <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
          
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
            
            <span class="n">train_dice</span> <span class="o">=</span> <span class="nf">dice</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

            
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

            <span class="n">train_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            <span class="n">avg_dice</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">train_dice</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            
            
        <span class="c1"># evaluate on validation set
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">val_loader</span><span class="p">):</span>
                <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
                
                <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
                <span class="n">val_dice</span> <span class="o">=</span> <span class="nf">dice</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

                <span class="n">val_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">avg_dice_val</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">val_dice</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            
            <span class="n">val_loss_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
            <span class="n">avg_dice_val_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">avg_dice_val</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">avg_dice_val</span><span class="p">)</span>

       
        <span class="n">train_loss_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
        <span class="n">avg_dice_epoch</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">avg_dice</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">avg_dice</span><span class="p">)</span>

        <span class="c1"># print average losses and dice coefficients for the epoch
</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s"> | Train Loss: </span><span class="si">{</span><span class="n">train_loss_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> | Val Loss: </span><span class="si">{</span><span class="n">val_loss_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> |Dice Coefficient: </span><span class="si">{</span><span class="n">avg_dice_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> | Val Dice Coefficient: </span><span class="si">{</span><span class="n">avg_dice_val_epoch</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <p>By following this training process, we unlock the potential of deep learning to achieve accurate and reliable segmentation on the CAD-PE dataset. The model progressively learns to differentiate and classify the structures of interest.</p> <h2 id="evaluating-the-results">Evaluating the Results</h2> <p>After training the UNet model on the CAD-PE dataset, it is time to evaluate the achieved results. The Dice coefficient, a widely used metric for segmentation tasks, provides a quantitative measure of the model’s performance.</p> <p>On the training set, after <code class="language-plaintext highlighter-rouge">training for 30 epochs</code>, the UNet model achieved an impressive Dice coefficient of <strong>0.88197</strong>. This indicates a significant overlap between the predicted segmentation masks and the ground truth masks, highlighting the model’s ability to accurately capture the target structures within the CAD-PE slices.</p> <p>To further assess the generalization capability of the model, it is essential to evaluate its performance on the test set. On the <code class="language-plaintext highlighter-rouge">test set</code>, the UNet model attained a Dice coefficient of <strong>0.70431</strong>. Although slightly lower than the training set performance, it still demonstrates the model’s effectiveness in accurately segmenting the target structures in previously unseen data.</p> <p>To improve the model’s performance, several avenues can be explored. Firstly, hyperparameter tuning can be conducted by adjusting parameters such as learning rate, batch size, and regularization techniques. Fine-tuning these hyperparameters can potentially lead to better segmentation results.</p> <p>Additionally, data augmentation techniques can be employed to augment the training set. Techniques such as random rotations, translations, and scaling can increase the diversity of the training data, enhancing the model’s ability to generalize to unseen samples.</p> <p>Visualizing the model’s performance, the predicted segmentation masks can be compared side-by-side with the original ground truth masks.</p> <div style="text-align: center;"> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/predicted_result-480.webp 480w,/assets/img/blog1/predicted_result-800.webp 800w,/assets/img/blog1/predicted_result-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog1/predicted_result.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Ground masks and Predicted masks. </div> <p>Overall, the results achieved by the UNet model showcase its potential in CAD-PE segmentation. With further experimentation, fine-tuning of hyperparameters, and augmentation of the training data, it is possible to unlock even better segmentation results. The UNet model serves as a foundation for future advancements in segmentation.</p>]]></content><author><name>Anirudh Iyengar</name></author><category term="computer_vision"/><category term="image_processing"/><category term="AI"/><category term="ML"/><category term="Medical_images"/><summary type="html"><![CDATA[The segmentation is performed on CAD-PE dataset]]></summary></entry><entry><title type="html">ನನ್ನ ಕವಿತೆಗಳು-ಭಾವನೆಗಳ ಕನಸುಗಳು</title><link href="https://anirudh6415.github.io/blog/2023/kavithegalu/" rel="alternate" type="text/html" title="ನನ್ನ ಕವಿತೆಗಳು-ಭಾವನೆಗಳ ಕನಸುಗಳು"/><published>2023-05-01T00:00:00+00:00</published><updated>2023-05-01T00:00:00+00:00</updated><id>https://anirudh6415.github.io/blog/2023/kavithegalu</id><content type="html" xml:base="https://anirudh6415.github.io/blog/2023/kavithegalu/"><![CDATA[<h3 id="ಕ್ಷಣಕ್ಕೆ-ಓಮ್ಮೆ">ಕ್ಷಣಕ್ಕೆ ಓಮ್ಮೆ</h3> <p>ಕ್ಷಣಕ್ಕೆ ಓಮ್ಮೆ ನಾ ಹುಟ್ಟುತ್ತಿರುವೆ <br/> ನಿನ್ನ ನೋಡಿದ ಕ್ಷಣದಿಂದ <br/> <br/> ಕ್ಷಣಕ್ಕೆ ಓಮ್ಮೆ ನಾ ಸಾಯುತ್ತಿರುವೆ <br/> ನೀ ಹೊರಟ ಕ್ಷಣದಿಂದ <br/> <br/> ಇದೆಂಥ ವಿಪರ್ಯಾಸವೋ..!<br/> ಇದೆಂಥ ಖುಷಿಯ ಶಾಪವೋ…!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/11-480.webp 480w,/assets/img/11-800.webp 800w,/assets/img/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="life"/><summary type="html"><![CDATA[ಈ ಬ್ಲಾಗ್ ಪೋಸ್ಟ್‌ನಲ್ಲಿ ನೀವು ಕನಸುಗಳ ಸ್ವರೂಪದ ಕವಿತೆಗಳನ್ನು ಕಂಡುಹಿಡಿಯುವ ಅನುಭವಕ್ಕೆ ಕರೆದೊಯ್ಯುತ್ತೇವೆ]]></summary></entry></feed>